# -*- coding: utf-8 -*-
"""Proyecto1AnaliticaV1Dash.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSxYNbV4bPicIXZOcQtUXceChHP_LDfI
"""

import plotly.express as px
import pandas as pd
df = pd.read_excel("C:/Users/sarit/OneDrive/Documentos/GitHub/Proyecto1_Analitica/Parte 1/data_variables.xlsx")

fig = target_distribution = df['target'].value_counts().reset_index()
target_distribution.columns = ['target', 'count']

# Crear un gráfico de torta
fig = px.pie(target_distribution, names='target', values='count',
             title='Distribución de la Variable "target"')
fig.show()

import plotly.express as px
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'df' con tus datos
# Asegúrate de que 'df' contiene la columna 'target'

# Reemplaza 'df' con tu propio DataFrame
df = pd.read_excel("C:/Users/sarit/OneDrive/Documentos/GitHub/Proyecto1_Analitica/Parte 1/data_variables.xlsx")

# Crea un histograma de la variable "target"
fig = px.histogram(df, x="target", title="Histograma de la Variable 'target'")
fig.show()

import plotly.express as px
import pandas as pd
import dash
import dash_core_components as dcc
from dash import html
from dash.dependencies import Input, Output
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.factors.discrete import TabularCPD
from pgmpy.sampling import BayesianModelSampling
from pgmpy.estimators import MaximumLikelihoodEstimator
import numpy
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from pgmpy.estimators import HillClimbSearch
from pgmpy.estimators import K2Score
from dash import dcc  # dash core components
from dash import html # dash html components
from dash.dependencies import Input, Output
import psycopg2
from dotenv import load_dotenv # pip install python-dotenv
import os
from pgmpy.readwrite import BIFWriter

# path to env file
env_path="C:\\Users\\sarit\\proy2\\app.env"
# load env 
load_dotenv(dotenv_path=env_path)
# extract env variables
USER=os.getenv('USER')
PASSWORD=os.getenv('PASSWORD')
HOST=os.getenv('HOST')
PORT=os.getenv('PORT')
DBNAME=os.getenv('DBNAME')

#connect to DB
engine = psycopg2.connect(
    dbname=DBNAME,
    user=USER,
    password=PASSWORD,
    host=HOST,
    port=PORT
)
print(DBNAME)
print(USER)
print(PASSWORD)
print(HOST)
print(PORT)

cursor = engine.cursor()
query = "SELECT * FROM variables"
#crear modelo
df=pd.read_sql(query, engine)

# Supongamos que tienes un DataFrame llamado 'df' con tus datos
# Asegúrate de que 'df' contiene las columnas 'target' y 'course'

# Reemplaza 'df' con tu propio DataFrame
#df = pd.read_excel("C:/Users/sarit/OneDrive/Documentos/GitHub/Proyecto1_Analitica/Parte 1/data_variables.xlsx")

# Inicializa la aplicación Dash
app = dash.Dash(__name__)

# Define la disposición de la aplicación
app.layout = html.Div([
    dcc.Checklist(
        id='target-checklist',
        options=[{'label': target, 'value': target} for target in df['target'].unique()],
        value=df['target'].unique(),  # Valor inicial (todos los valores seleccionados)
        labelStyle={'display': 'block'}  # Mostrar etiquetas en bloques para una mejor visualización
    ),
    dcc.Graph(id='target-graph')
])

# Define la función de actualización del gráfico
@app.callback(
    Output('target-graph', 'figure'),
    Input('target-checklist', 'value')
)
def update_graph(selected_targets):
    filtered_df = df[df['target'].isin(selected_targets)]
    fig = px.bar(filtered_df, x="course", color="target", title="Distribución de 'target' por Carrera")
    return fig

# Ejecuta la aplicación Dash
if __name__ == '__main__':
    app.run_server(debug=True)
